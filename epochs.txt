----------Epoch 0----------
Loss: 3.6317697 Train acc: 0.11956522
----------Epoch 1----------
Loss: 3.5283744 Train acc: 0.13695653
----------Epoch 2----------
Loss: 3.2853787 Train acc: 0.19347826
----------Epoch 3----------
Loss: 3.0736642 Train acc: 0.22391304
----------Epoch 4----------
Loss: 2.9060955 Train acc: 0.24130435
----------Epoch 5----------
Loss: 2.82739 Train acc: 0.2521739
----------Epoch 6----------
Loss: 2.5660543 Train acc: 0.3152174
----------Epoch 7----------
Loss: 2.2828553 Train acc: 0.39347827
----------Epoch 8----------
Loss: 2.142305 Train acc: 0.3847826
----------Epoch 9----------
Loss: 1.9126537 Train acc: 0.45869565
----------Epoch 10----------
Loss: 1.7404027 Train acc: 0.5217391
----------Epoch 11----------
Loss: 1.6945903 Train acc: 0.5043478
----------Epoch 12----------
Loss: 1.5458263 Train acc: 0.57391304
----------Epoch 13----------
Loss: 1.5475007 Train acc: 0.5586957
----------Epoch 14----------
Loss: 1.4009651 Train acc: 0.59565216
----------Epoch 15----------
Loss: 1.3126006 Train acc: 0.62391305
----------Epoch 16----------
Loss: 1.2593561 Train acc: 0.65
----------Epoch 17----------
Loss: 1.1762521 Train acc: 0.6869565
----------Epoch 18----------
Loss: 1.0551769 Train acc: 0.6978261
----------Epoch 19----------
Loss: 0.9800306 Train acc: 0.70869565
----------Epoch 20----------
Loss: 0.9882847 Train acc: 0.71304345
----------Epoch 21----------
Loss: 0.8867709 Train acc: 0.75652176
----------Epoch 22----------
Loss: 0.8690357 Train acc: 0.76304346
----------Epoch 23----------
Loss: 0.7408347 Train acc: 0.7978261
----------Epoch 24----------
Loss: 0.7370277 Train acc: 0.8065217
----------Epoch 25----------
Loss: 0.64202857 Train acc: 0.84347826
----------Epoch 26----------
Loss: 0.6312934 Train acc: 0.84130436
----------Epoch 27----------
Loss: 0.5604706 Train acc: 0.8586956
----------Epoch 28----------
Loss: 0.59653693 Train acc: 0.8652174
----------Epoch 29----------
Loss: 0.5352646 Train acc: 0.87391305
----------Epoch 30----------
Loss: 0.48246142 Train acc: 0.90869564
----------Epoch 31----------
Loss: 0.41572514 Train acc: 0.91086954
----------Epoch 32----------
Loss: 0.41312972 Train acc: 0.9173913
----------Epoch 33----------
Loss: 0.39662218 Train acc: 0.9195652
----------Epoch 34----------
Loss: 0.3625954 Train acc: 0.93043476
----------Epoch 35----------
Loss: 0.32985228 Train acc: 0.9434783
----------Epoch 36----------
Loss: 0.3343746 Train acc: 0.9347826
----------Epoch 37----------
Loss: 0.29851776 Train acc: 0.96086955
----------Epoch 38----------
Loss: 0.25281724 Train acc: 0.9717391
----------Epoch 39----------
Loss: 0.26167485 Train acc: 0.9695652
----------Epoch 40----------
Loss: 0.31959075 Train acc: 0.9521739
----------Epoch 41----------
Loss: 0.3206352 Train acc: 0.96304345
----------Epoch 42----------
Loss: 0.2539907 Train acc: 0.9695652
----------Epoch 43----------
Loss: 0.21290648 Train acc: 0.98913044
----------Epoch 44----------
Loss: 0.1726326 Train acc: 0.99130434
----------Epoch 45----------
Loss: 0.1438512 Train acc: 0.9978261
----------Epoch 46----------
Loss: 0.13895617 Train acc: 0.99347824
----------Epoch 47----------
Loss: 0.16329595 Train acc: 0.98695654
----------Epoch 48----------
Loss: 0.1436879 Train acc: 0.99130434
----------Epoch 49----------
Loss: 0.1451565 Train acc: 0.9956522
----------Epoch 50----------
Loss: 0.15374921 Train acc: 0.99347824
----------Epoch 51----------
Loss: 0.15121934 Train acc: 1.0
----------Epoch 52----------
Loss: 0.17537335 Train acc: 0.9826087
----------Epoch 53----------



class Discriminator(object):
    def __init__(self, config):
        self.name = "Discriminator"

    def __call__(self, x):
        with tf.compat.v1.variable_scope(self.name) as vs:
            x1 = slim.legacy_fully_connected(
                x=x, num_output_units=x.shape[1], activation_fn=tf.nn.tanh)
            x2 = slim.legacy_fully_connected(x=x1, num_output_units=int(
                x.shape[1])//2, activation_fn=tf.nn.tanh)
            predict_mask = slim.legacy_fully_connected(
                x=x2, num_output_units=x.shape[1], activation_fn=tf.nn.sigmoid)
        return predict_mask

    @property
    def vars(self):
        return [var for var in tf.compat.v1.global_variables() if self.name in var.name]


