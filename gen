class Generator(object):

    def __init__(self, config):
        self.batch_size = config.batch_size  # configurable
        self.hidden_size = config.hidden_size  # congigurable for GRU/LSTM
        self.num_steps = config.num_steps  # length of input array
        # dimension of input eleemnt array univariate 1
        self.input_dimension_size = config.input_dimension_size
        self.cell_type = config.cell_type  # RNN Cell type
        self.lamda = config.lamda  # coefficient that balances the prediction loss
        self.class_num = config.class_num  # number of targes
        self.layer_num = config.layer_num  # number of layers of AJRNN
        self.name = 'Generator_LSTM'

    def build_model(self):

        # input has shape (batch_size, n_steps, embedding_size)
        input = tf.compat.v1.placeholder(tf.float32, [
                                         self.batch_size, self.num_steps, self.input_dimension_size], name='inputs')  # input

        # prediction_target has shape (batch_size, n_steps-1, embedding_size) # TODO why -1?
        prediction_target = tf.compat.v1.placeholder(tf.float32, [
                                                     self.batch_size, self.num_steps - 1, self.input_dimension_size], name='prediction_target')
        mask = tf.compat.v1.placeholder(tf.float32, [
                                        self.batch_size, self.num_steps - 1, self.input_dimension_size], name='mask')

        # label_target has shape (batch_size, self.class_num) # likes one-hot
        label_target = tf.compat.v1.placeholder(
            tf.float32, [self.batch_size, self.class_num], name='label_target')

        # dropout for rnn
        lstm_keep_prob = tf.compat.v1.placeholder(
            tf.float32, [], name='lstm_keep_prob')
        classfication_keep_prob = tf.compat.v1.placeholder(
            tf.float32, [], name='classification_keep_prob')

        with tf.compat.v1.variable_scope(self.name):
            # project layer weight W and bias
            W = tf.Variable(tf.random.truncated_normal(
                [self.hidden_size, self.input_dimension_size], stddev=0.1), dtype=tf.float32, name='Project_W')
            bias = tf.Variable(tf.constant(
                0.1, shape=[self.input_dimension_size]), dtype=tf.float32, name='Project_bias')

            # construct cells with the specific layer_num
            mulrnn_cell = tf.keras.layers.StackedRNNCells([RNN_cell(
                type=self.cell_type, hidden_size=self.hidden_size, keep_prob=lstm_keep_prob) for _ in range(self.layer_num)])

            # initialize state to zero
            init_state = mulrnn_cell.get_initial_state(
                batch_size=self.batch_size, dtype=tf.float32)
            state = init_state

            outputs = list()

            # makes cell run
            # outputs has list of 'num_steps' with each element's shape (batch_size, hidden_size)
            with tf.compat.v1.variable_scope("RNN"):
                for time_step in range(self.num_steps):
                    if time_step > 0:
                        tf.compat.v1.get_variable_scope().reuse_variables()
                    if time_step == 0:
                        (cell_output, state) = mulrnn_cell(
                            input[:, time_step, :], state)
                        outputs.append(cell_output)
                    else:
                        # comparison has shape (batch_size, self.input_dimension_size) with elements 1 (means missing) when equal or 0 (not missing) otherwise
                        comparison = tf.equal(
                            input[:, time_step, :], tf.constant(Missing_value))
                        current_prediction_output = tf.matmul(
                            outputs[time_step - 1], W) + bias
                        # change the current_input, select current_prediction_output when 1 (missing) or use input when 0 (not missing)
                        current_input = tf.where(
                            comparison, current_prediction_output, input[:, time_step, :])
                        (cell_output, state) = mulrnn_cell(current_input, state)
                        outputs.append(cell_output)

            # label_target_hidden_output has the last_time_step of shape (batch_size, hidden_size)
            label_target_hidden_output = outputs[-1]

            # prediction_target_hidden_output has list of 'num_steps - 1' with each element's shape (batch_size, hidden_size)
            prediction_target_hidden_output = outputs[:-1]

            # unfolded outputs into the [batch, hidden_size * (numsteps-1)], and then reshape it into [batch * (numsteps-1), hidden_size]
            prediction_hidden_output = tf.reshape(tensor=tf.concat(
                values=prediction_target_hidden_output, axis=1), shape=[-1, self.hidden_size])

            # prediction has shape (batch * (numsteps - 1), self.input_dimension_size)
            prediction = tf.add(
                tf.matmul(prediction_hidden_output, W), bias, name='prediction')

            # reshape prediction_target and corresponding mask  into [batch * (numsteps-1), hidden_size]
            prediction_targets = tf.reshape(
                prediction_target, [-1, self.input_dimension_size])
            masks = tf.reshape(mask, [-1, self.input_dimension_size])

            #  softmax for the label_prediction, label_logits has shape (batch_size, self.class_num)
            with tf.compat.v1.variable_scope('Softmax_layer'):
                label_logits = layers.Dense(self.class_num)(
                    label_target_hidden_output)
                loss_classficiation = tf.nn.softmax_cross_entropy_with_logits(
                    labels=tf.stop_gradient(label_target), logits=label_logits, name='loss_classficiation')

        # use mask to use the observer values for the loss_prediction
        with tf.compat.v1.name_scope("loss_prediction"):
            loss_prediction = tf.reduce_mean(input_tensor=tf.square(
                (prediction_targets - prediction) * masks)) / (self.batch_size)

        regularization_loss = 0.0
        for i in self.vars:
            regularization_loss += tf.nn.l2_loss(i)

        with tf.compat.v1.name_scope("loss_total"):
            loss = loss_classficiation + self.lamda * \
                loss_prediction + 1e-4 * regularization_loss

        # for get the classfication accuracy, label_predict has shape (batch_size, self.class_num)
        label_predict = tf.nn.softmax(label_logits, name='test_probab')
        correct_predictions = tf.equal(
            tf.argmax(input=label_predict, axis=1), tf.argmax(input=label_target, axis=1))
        accuracy = tf.cast(correct_predictions, tf.float32, name='accuracy')

        input_tensors = {
            'input': input,
            'prediction_target': prediction_target,
            'mask': mask,
            'label_target': label_target,
            'lstm_keep_prob': lstm_keep_prob,
            'classfication_keep_prob': classfication_keep_prob
        }

        loss_tensors = {
            'loss_prediction': loss_prediction,
            'loss_classficiation': loss_classficiation,
            'regularization_loss': regularization_loss,
            'loss': loss
        }

        prediction_res = tf.reshape(
            prediction, [-1, (self.num_steps - 1)*self.input_dimension_size])
        M = tf.reshape(mask, [-1, (self.num_steps - 1)
                       * self.input_dimension_size])

        return input_tensors, loss_tensors, accuracy, prediction_res, M, label_predict, tf.reshape(prediction_targets, [-1, (self.num_steps - 1)*self.input_dimension_size]), label_target_hidden_output

    @property
    def vars(self):
        return [var for var in tf.compat.v1.global_variables() if self.name in var.name]


class ConditionalGAN(keras.Model):
    def __init__(self, discriminator, generator, latent_dim):
        super(ConditionalGAN, self).__init__()
        self.discriminator = discriminator
        self.generator = generator
        self.latent_dim = latent_dim
        self.gen_loss_tracker = keras.metrics.Mean(name="generator_loss")
        self.disc_loss_tracker = keras.metrics.Mean(name="discriminator_loss")

    @property
    def metrics(self):
        return [self.gen_loss_tracker, self.disc_loss_tracker]

    def compile(self, d_optimizer, g_optimizer, loss_fn):
        super(ConditionalGAN, self).compile()
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer
        self.loss_fn = loss_fn

    def train_step(self, data):
        # Unpack the data.
        real_images, one_hot_labels = data

        # Add dummy dimensions to the labels so that they can be concatenated with
        # the images. This is for the discriminator.
        image_one_hot_labels = one_hot_labels[:, :, None, None]
        image_one_hot_labels = tf.repeat(
            image_one_hot_labels, repeats=[image_size * image_size]
        )
        image_one_hot_labels = tf.reshape(
            image_one_hot_labels, (-1, image_size, image_size, num_classes)
        )

        # Sample random points in the latent space and concatenate the labels.
        # This is for the generator.
        batch_size = tf.shape(real_images)[0]
        random_latent_vectors = tf.random.normal(
            shape=(batch_size, self.latent_dim))
        random_vector_labels = tf.concat(
            [random_latent_vectors, one_hot_labels], axis=1
        )

        # Decode the noise (guided by labels) to fake images.
        generated_images = self.generator(random_vector_labels)

        # Combine them with real images. Note that we are concatenating the labels
        # with these images here.
        fake_image_and_labels = tf.concat(
            [generated_images, image_one_hot_labels], -1)
        real_image_and_labels = tf.concat(
            [real_images, image_one_hot_labels], -1)
        combined_images = tf.concat(
            [fake_image_and_labels, real_image_and_labels], axis=0
        )

        # Assemble labels discriminating real from fake images.
        labels = tf.concat(
            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0
        )

        # Train the discriminator.
        with tf.GradientTape() as tape:
            predictions = self.discriminator(combined_images)
            d_loss = self.loss_fn(labels, predictions)
        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)
        self.d_optimizer.apply_gradients(
            zip(grads, self.discriminator.trainable_weights)
        )

        # Sample random points in the latent space.
        random_latent_vectors = tf.random.normal(
            shape=(batch_size, self.latent_dim))
        random_vector_labels = tf.concat(
            [random_latent_vectors, one_hot_labels], axis=1
        )

        # Assemble labels that say "all real images".
        misleading_labels = tf.zeros((batch_size, 1))

        # Train the generator (note that we should *not* update the weights
        # of the discriminator)!
        with tf.GradientTape() as tape:
            fake_images = self.generator(random_vector_labels)
            fake_image_and_labels = tf.concat(
                [fake_images, image_one_hot_labels], -1)
            predictions = self.discriminator(fake_image_and_labels)
            g_loss = self.loss_fn(misleading_labels, predictions)
        grads = tape.gradient(g_loss, self.generator.trainable_weights)
        self.g_optimizer.apply_gradients(
            zip(grads, self.generator.trainable_weights))

        # Monitor loss.
        self.gen_loss_tracker.update_state(g_loss)
        self.disc_loss_tracker.update_state(d_loss)
        return {
            "g_loss": self.gen_loss_tracker.result(),
            "d_loss": self.disc_loss_tracker.result(),
        }
